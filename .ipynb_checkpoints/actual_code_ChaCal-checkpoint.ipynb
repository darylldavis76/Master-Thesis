{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import stft, welch\n",
    "from scipy.stats import kurtosis, skew\n",
    "from scipy.fft import fft, fftfreq\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle, parallel_backend\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "# Set the renderer to browser\n",
    "pio.renderers.default = 'browser'\n",
    "\n",
    "# Assuming `best_rfecv` is the object from RFECV and `X_train` is your training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Function to compute time features\n",
    "def compute_time_features(data, sample_rate):\n",
    "    features = {}\n",
    "    features['mean'] = np.mean(data)\n",
    "    features['RMS'] = np.sqrt(np.mean(np.square(data)))\n",
    "    features['StandardDeviation'] = np.std(data)\n",
    "    features['ShapeFactor'] = np.sqrt(np.mean(np.square(data))) / np.mean(np.abs(data))\n",
    "    features['SNR'] = np.mean(data) / np.std(data)\n",
    "    features['THD'] = np.sqrt(np.sum(np.square(data[1:])) / np.square(data[0])) # Total Harmonic Distortion\n",
    "    features['SINAD'] = np.mean(data) / np.sqrt(np.mean(np.square(data - np.mean(data))))\n",
    "    features['PeakValure'] = np.max(np.abs(data))\n",
    "    features['CrestFactor'] = np.max(np.abs(data)) / np.sqrt(np.mean(np.square(data)))\n",
    "    features['ClearanceFactor'] = np.max(np.abs(data)) / np.mean(np.sqrt(np.abs(data)))\n",
    "    features['ImpulseFactor'] = np.max(np.abs(data)) / np.mean(np.abs(data))\n",
    "    return features\n",
    "\n",
    "# Function to compute frequency features\n",
    "def compute_frequency_features(data, sample_rate):\n",
    "    freq_domain = np.abs(fft(data))\n",
    "    freqs = fftfreq(len(data), 1 / sample_rate)\n",
    "    features = {}\n",
    "    features['MeanFrequency'] = np.mean(freq_domain)\n",
    "    features['MedianFrequency'] = np.median(freq_domain)\n",
    "    features['BandPower'] = np.sum(freq_domain ** 2)\n",
    "    features['OccupiedBandwidth'] = np.sum(freq_domain > 0.05 * np.max(freq_domain))\n",
    "    features['PowerBandwidth'] = np.sum(freq_domain > 0.5 * np.max(freq_domain))\n",
    "    features['PeakAmplitude'] = np.max(freq_domain)\n",
    "    features['PeakLocation'] = freqs[np.argmax(freq_domain)]\n",
    "    return features\n",
    "\n",
    "# Function to compute time-frequency features using STFT\n",
    "def compute_time_freq_features(data, sample_rate):\n",
    "    f, t, Zxx = stft( data, fs=sample_rate, nperseg=256)\n",
    "    magnitude = np.abs(Zxx)\n",
    "\n",
    "    features={}\n",
    "    features['SpectralKurtosis'] = kurtosis(magnitude, axis=None)\n",
    "    features['SpectralSkewness'] = skew(magnitude, axis=None)\n",
    "    features['SpectralCrest'] = np.max(magnitude) / np.mean(magnitude)\n",
    "    features['SpectralFlatness'] = np.exp(np.mean(np.log(magnitude))) / np.mean(magnitude)\n",
    "    features['SpectralEntropy'] = -np.sum(magnitude * np.log2(magnitude), axis=None)\n",
    "    features['SpectralCentroid'] = np.sum(f[:, np.newaxis] * magnitude, axis=0) / np.sum(magnitude, axis=0)\n",
    "    features['SpectralSpread'] = np.sqrt(np.sum((f[:, np.newaxis] - features['SpectralCentroid'])**2 * magnitude, axis=0) / np.sum(magnitude, axis=0))\n",
    "    features['SpectralRolloff'] = np.sum(magnitude, axis=0)[np.newaxis] * 0.85\n",
    "    features['TFRidges'] = np.argmax(magnitude, axis=0)\n",
    "    features['InstantaneousBandwidth'] = np.std(magnitude, axis=0)\n",
    "    features['InstantaneousFrequency'] = np.mean(magnitude, axis=0)\n",
    "    features['MeanEnvelopeEnergy'] = np.mean(np.abs(magnitude), axis=0)\n",
    "    features['WaveletEntropy'] = -np.sum(np.square(magnitude) * np.log2(np.square(magnitude)), axis=None)\n",
    "    return features\n",
    "\n",
    "# Function to aggregate features\n",
    "def aggregate_features(vector_data):\n",
    "    aggregated_features = {}\n",
    "    for key, vec in vector_data.items():\n",
    "        aggregated_features[f'{key}_mean'] = np.mean(vec)\n",
    "        aggregated_features[f'{key}_std'] = np.std(vec)\n",
    "        aggregated_features[f'{key}_min'] = np.min(vec)\n",
    "        aggregated_features[f'{key}_max'] = np.max(vec)\n",
    "        aggregated_features[f'{key}_range'] = np.ptp(vec)\n",
    "    return aggregated_features\n",
    "\n",
    "# Function to normalize features\n",
    "def normalize_features(data):\n",
    "    return (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n",
    "\n",
    "# Main function to extract features from all CSV files in a folder\n",
    "def extract_m_features(folder_path):\n",
    "    feature_table = pd.DataFrame()\n",
    "    sample_rate = 1428.57\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            data = np.loadtxt(file_path, delimiter=',')\n",
    "\n",
    "            time_features = compute_time_features(data, sample_rate)\n",
    "            frequency_features = compute_frequency_features(data, sample_rate)\n",
    "            time_frequency_features = compute_time_freq_features(data, sample_rate)\n",
    "\n",
    "            aggregated_time_frequency_features = aggregate_features(time_frequency_features)\n",
    "\n",
    "            combined_features = {**time_features, **frequency_features, **aggregated_time_frequency_features}\n",
    "\n",
    "            combined_feature_table = pd.DataFrame([combined_features])\n",
    "            \n",
    "            feature_table = pd.concat([feature_table, combined_feature_table], ignore_index=True)\n",
    "\n",
    "    feature_table = feature_table.apply(normalize_features, axis=0)\n",
    "    feature_table['HealthState'] = 2\n",
    "\n",
    "    return feature_table\n",
    "\n",
    "\n",
    "folder_path = \"D:\\MT dataset\\mcsadc-IM motor-rotorbarfailure-2023\\mcsadc-IM-motor-rotorbarfailure-OG\\processed_CSV\\SC_abc_2BB\"\n",
    "BB2_feature_table = extract_m_features(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "combined_feature_table = pd.concat([healthy_feature_table, BB1_feature_table, BB2_feature_table], ignore_index=True)\n",
    "X = combined_feature_table.drop(columns=['HealthState'])\n",
    "y = combined_feature_table['HealthState']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Debug statement to print the normalized data\n",
    "print('Normalized X data:')\n",
    "print(X_normalized)\n",
    "\n",
    "print('Y_numeric data:')\n",
    "print(y)\n",
    "\n",
    "# now 'X_normalized' contains the normalized features and 'y' contains the labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Define data augmentation functions\n",
    "def add_noise(data, noise_level=0.01):\n",
    "    noise = noise_level * np.random.normal(size=data.shape)\n",
    "    return data + noise\n",
    "\n",
    "def scale(data, scaling_factor=1.1):\n",
    "    return data * scaling_factor\n",
    "\n",
    "def time_shift(data, shift_max=2):\n",
    "    shift = np.random.randint(-shift_max, shift_max)\n",
    "    return np.roll(data, shift)\n",
    "\n",
    "def augment_data(X, Y, augmentations=5):\n",
    "    augmented_X, augmented_Y = [], []\n",
    "    for _ in range(augmentations):\n",
    "        for x, y in zip(X, Y):\n",
    "            augmented_X.append(add_noise(x))\n",
    "            augmented_X.append(scale(x))\n",
    "            augmented_X.append(time_shift(x))\n",
    "            augmented_Y.extend([y, y, y])\n",
    "    return np.array(augmented_X), np.array(augmented_Y)\n",
    "\n",
    "# Assuming X_normalized and y are already defined\n",
    "# Augment the dataset\n",
    "augmented_X, augmented_Y = augment_data(X_normalized, y)\n",
    "\n",
    "# Combine original and augmented data\n",
    "X_final = np.vstack((X_normalized, augmented_X))\n",
    "y_final = np.hstack((y, augmented_Y))\n",
    "\n",
    "# Shuffle the data\n",
    "X_final, y_final = shuffle(X_final, y_final, random_state=42)\n",
    "\n",
    "# Impute missing values with mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_final = imputer.fit_transform(X_final)\n",
    "\n",
    "# Debug statement to print the normalized data\n",
    "print('Final X data:')\n",
    "print(X_final)\n",
    "\n",
    "print('Final y:')\n",
    "print(y_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Using Recursive Feature Elimination with Grid Search Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model and parameter grid for SVM\n",
    "svm_model = SVC()\n",
    "svm_param_grid = {\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Define the model and parameter grid for kNN\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "# Define the parameter grid for LinearSVC within RFECV\n",
    "param_grid = {\n",
    "    'estimator__C': [10, 20, 30, 40, 50, 100]\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=RFECV(estimator=LinearSVC(max_iter=10000), step=5, cv=5, scoring='accuracy', n_jobs=-1, verbose=2),\n",
    "                           param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=3)\n",
    "\n",
    "# Create the RFE (Recursive Feature Elimination) model with cross-validation for SVM with a linear kernel\n",
    "# linear_svm = LinearSVC(max_iter=10000)\n",
    "# Adjusting step parameter to reduce iterations\n",
    "# rfecv = RFECV(estimator=linear_svm, step=2, cv=5, scoring='accuracy', min_features_to_select=15, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the RFECV model\n",
    "with parallel_backend('threading'):\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best RFECV estimator\n",
    "best_rfecv = grid_search.best_estimator_\n",
    "\n",
    "# Get the selected features for SVM    \n",
    "selected_features_svm = best_rfecv.support_\n",
    "\n",
    "# Plot the number of features selected vs cross-validated score\n",
    "fig1 = go.Figure()\n",
    "fig1.add_trace(go.Scatter(\n",
    "    x=list(range(1, len(best_rfecv.cv_results_[\"mean_test_score\"]) + 1)),\n",
    "    y=best_rfecv.cv_results_[\"mean_test_score\"],\n",
    "    mode='lines+markers',\n",
    "    marker=dict(symbol='circle', size=8),\n",
    "    line=dict(dash='dash')\n",
    "))\n",
    "\n",
    "fig1.update_layout(\n",
    "    title='RFECV - No. of features selected vs Cross-Validated Score',\n",
    "    xaxis_title='No. of features selected',\n",
    "    yaxis_title='Cross-Validated Score (Accuracy)',\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "fig1.show()\n",
    "\n",
    "\n",
    "\n",
    "# Plot the ranking of features\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.title('RFECV - Feature Ranking')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Rank')\n",
    "plt.bar(range(X_train.shape[1]), best_rfecv.ranking_, color='b', align='center')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plot the support of features\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.title('RFECV - Feature Support')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Selected (1) / Not Selected (0)')\n",
    "plt.bar(range(X_train.shape[1]), best_rfecv.support_, color='r', align='center')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Check the number of selected features\n",
    "num_selected_features_svm = np.sum(selected_features_svm)\n",
    "print(f\"Number of selected features with optimized Linear SVC: {num_selected_features_svm}\")\n",
    "\n",
    "# Transform the training and test sets with the selected features\n",
    "X_train_selected_svm = best_rfecv.transform(X_train)\n",
    "X_test_selected_svm = best_rfecv.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Using Principal Component Analysis for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Using PCA to reduce the number of features\n",
    "n_components=20\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Print the number of features selected by PCA\n",
    "print(f\"Number of features after PCA: {n_components}\")\n",
    "\n",
    "# Plot the explained variance ratio to see the selection criteria\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='--')\n",
    "plt.title('Explained Variance by PCA components')\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Coarse and detailed Grid Search SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "\n",
    "# Create the pipeline with imputer and feature selection for SVM    # This is removed for now\n",
    "# pipeline_svm = Pipeline([\n",
    "    # ('imputer', SimpleImputer(strategy='mean')),\n",
    "    # ('rfe', rfecv_svm),\n",
    "# ])\n",
    "\n",
    "# Coarse grid search to find the most promising hyperparameters\n",
    "coarse_grid_search_svm = GridSearchCV(estimator=svm_model, param_grid=svm_param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=3)\n",
    "coarse_grid_search_svm.fit(X_train_selected_svm, y_train)\n",
    "\n",
    "# Narrow down the range for detailed search and USE THE BEST PARAMETERS from the coarse grid search for a more detailed search\n",
    "best_params_svm = coarse_grid_search_svm.best_params_\n",
    "detailed_param_grid_svm = {\n",
    "    'kernel': [best_params_svm['kernel']],\n",
    "    'C': [best_params_svm['C']/2, best_params_svm['C'], best_params_svm['C']*2],\n",
    "    'gamma': [best_params_svm['gamma']]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Perform a detailed grid search\n",
    "detailed_grid_search_svm = GridSearchCV(estimator=svm_model, param_grid=detailed_param_grid_svm, cv=5, scoring='accuracy', n_jobs=-1, verbose=3)\n",
    "detailed_grid_search_svm.fit(X_train_selected_svm, y_train)\n",
    "\n",
    "# Get the best SVM model\n",
    "svm_best_estimator = detailed_grid_search_svm.best_estimator_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Evaluate SVM model on the test set\n",
    "y_pred_svm = svm_best_estimator.predict(X_test_selected_svm)\n",
    "svm_accuracy = accuracy_score(y_test, y_pred_svm)\n",
    "\n",
    "print(\"SVM selected features:\", selected_features_svm)\n",
    "print(\"SVM test accuracy:\", svm_accuracy*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Create the RFE model with cross-validation for kNN\n",
    "rfecv_knn = RFECV(estimator=knn_model, step=5, cv=5, scoring='accuracy',n_jobs=-1, min_features_to_select=15, verbose=3)\n",
    "\n",
    "# Perform grid search on the pipeline for kNN\n",
    "grid_search_knn = GridSearchCV(estimator=rfecv_knn, param_grid=knn_param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=3)\n",
    "\n",
    "# Fit the RFECV model\n",
    "with parallel_backend('threading'):\n",
    "    grid_search_knn.fit(X_train, y_train)\n",
    "\n",
    "# Get the best RFECV estimator\n",
    "knn_best_rfecv = grid_search_knn.best_estimator_\n",
    "\n",
    "# Plot the number of features selected versus cross-validated score\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('RFECV - No.of features selected vs Cross-Validated Score')\n",
    "plt.xlabel('No. of features selected')\n",
    "plt.ylabel('Cross-Validated Score (Accuracy)')\n",
    "plt.plot(range(1, len(knn_best_rfecv.cv_results_[\"mean_test_score\"]) + 1), knn_best_rfecv.cv_results_[\"mean_test_score\"], marker='o', linestyle='--')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plot the ranking of features\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.title('RFECV - Feature Ranking')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Rank')\n",
    "plt.bar(range(X_train.shape[1]), knn_best_rfecv.ranking_, color='b', align='center')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plot the support of features\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.title('RFECV - Feature Support')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Selected (1) / Not Selected (0)')\n",
    "plt.bar(range(X_train.shape[1]), knn_best_rfecv.support_, color='r', align='center')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Get the selected features for kNN\n",
    "selected_features_knn = knn_best_rfecv.support_\n",
    "\n",
    "# Evaluate kNN model on the test set\n",
    "X_test_selected_knn = rfecv_knn.transform(X_test)\n",
    "knn_predictions = knn_best_estimator.predict(X_test_selected_knn)\n",
    "knn_accuracy = accuracy_score(y_test, knn_predictions)\n",
    "\n",
    "print(\"kNN selected features:\", selected_features_knn)\n",
    "print(\"kNN test accuracy:\", knn_accuracy)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
