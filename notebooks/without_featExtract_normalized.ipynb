{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from multiprocessing import Pool, cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary classifiers \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from file 1:\n",
      "     Time    SC_a     SC_b     SC_c   Speed  Health State\n",
      "0  0.0000 -2.1436  1.61130  0.49563  312.45  1 Broken bar\n",
      "1  0.0007 -2.2021  1.42090  0.75081  312.44  1 Broken bar\n",
      "2  0.0014 -2.0996  1.00100  1.08450  312.44  1 Broken bar\n",
      "3  0.0021 -1.9043  0.37109  1.52120  312.44  1 Broken bar\n",
      "4  0.0028 -1.6895 -0.22461  1.89420  312.42  1 Broken bar\n",
      "Data from file1 processed successfully.\n",
      "Data from file 2:\n",
      "     Time      SC_a     SC_b    SC_c   Speed   Health State\n",
      "0  0.0000  1.391600  0.63477 -2.0071  312.25  2 Broken bars\n",
      "1  0.0007  1.054700  0.96680 -2.0120  312.24  2 Broken bars\n",
      "2  0.0014  0.561520  1.34770 -1.9138  312.24  2 Broken bars\n",
      "3  0.0021 -0.019531  1.73830 -1.7224  312.23  2 Broken bars\n",
      "4  0.0028 -0.493160  2.04100 -1.5556  312.21  2 Broken bars\n",
      "Data from file2 processed successfully.\n",
      "Data from file 3:\n",
      "     Time      SC_a     SC_b    SC_c   Speed Health State\n",
      "0  0.0000  1.391600  0.63477 -2.0071  312.25      Healthy\n",
      "1  0.0007  1.054700  0.96680 -2.0120  312.24      Healthy\n",
      "2  0.0014  0.561520  1.34770 -1.9138  312.24      Healthy\n",
      "3  0.0021 -0.019531  1.73830 -1.7224  312.23      Healthy\n",
      "4  0.0028 -0.493160  2.04100 -1.5556  312.21      Healthy\n",
      "Data from file3 processed successfully.\n",
      "Normalized data:\n",
      "[[-1.73192104 -1.15254386  0.89424047  0.27897835  0.94859423]\n",
      " [-1.7316615  -1.18375292  0.78981016  0.42383768  0.94647251]\n",
      " [-1.73140196 -1.12907038  0.55950403  0.61326518  0.94647251]\n",
      " ...\n",
      " [ 1.73140196 -1.1551046   1.3441574  -0.18901867 -0.4750814 ]\n",
      " [ 1.7316615  -1.31141665  1.14867966  0.18148015 -0.47932485]\n",
      " [ 1.73192104 -1.42600992  0.91299844  0.53526095 -0.48356829]]\n",
      "all_Y_numeric:\n",
      "[1. 1. 1. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Define the folder containing your csv files\n",
    "folder_path = \"D:\\MT dataset\\mcsadc-IM motor-rotorbarfailure-2023\\combined files_training set\\Training data_with label\"\n",
    "\n",
    "# Get a list of all csv files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')] # creates a list of all filenames in the directory folder_path that end with .csv\n",
    "\n",
    "feat_vars = ['Time', 'SC_a', 'SC_b', 'SC_c', 'Speed']\n",
    "class_var = 'Health State'\n",
    "\n",
    "# initialize empty lists to store data from all the files\n",
    "all_X = []\n",
    "all_Y_numeric = []\n",
    "\n",
    "# loop through each csv file\n",
    "for i, file_name in enumerate(csv_files):\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    # Read data from the current csv file\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Debug statement to print the contents of the data\n",
    "    print(f'Data from file {i + 1}:')\n",
    "    print(data.head())\n",
    "\n",
    "    # Extract the predictor variables (features) and response variables (classes)\n",
    "    X = data[feat_vars].values # features\n",
    "    Y_str = data[class_var].values # String labels\n",
    "\n",
    "    # Initialize an empty array to store the encoded labels\n",
    "    Y_numeric = np.zeros(len(Y_str))\n",
    "\n",
    "    # Loop through each row of Y_str\n",
    "    for j in range(len(Y_str)):\n",
    "        # Debug statement to print the current value of Y_str\n",
    "        # print(f'Y_str[{j}]: {Y_str[j]}')\n",
    "\n",
    "        # Convert Y_str to lowercase for case-sensitive matching \n",
    "        lower_Y_str = Y_str[j].lower()\n",
    "\n",
    "        # Check for the presence of specific health states\n",
    "        if '1 broken bar' in lower_Y_str:\n",
    "            Y_numeric[j] = 1 # Encode as 1 for 1 broken bar\n",
    "        elif '2 broken bars' in lower_Y_str:\n",
    "            Y_numeric[j] = 2 # Encode as 2 for 2 broken bars\n",
    "        else:\n",
    "            Y_numeric[j] = 0 # Encode as 0 for healthy state\n",
    "\n",
    "    # Append data from the current file to the lists\n",
    "    all_X.append(X)\n",
    "    all_Y_numeric.append(Y_numeric)\n",
    "\n",
    "    # Debug statement to indicate successful processing\n",
    "    print(f'Data from file{i + 1} processed successfully.')\n",
    "    \n",
    "# Convert lists to numpy arrays\n",
    "all_X = np.vstack(all_X)\n",
    "all_Y_numeric = np.hstack(all_Y_numeric)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "all_X_normalized = scaler.fit_transform(all_X)\n",
    "\n",
    "# Debug statement to print the normalized data\n",
    "print('Normalized data:')\n",
    "print(all_X_normalized)\n",
    "\n",
    "print('all_Y_numeric:')\n",
    "print(all_Y_numeric)\n",
    "\n",
    "# now 'all_X_normalized' contains the normalized features and 'all_Y_numeric' contains the labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data augmentation functions\n",
    "def add_noise(data, noise_level=0.01):\n",
    "    noise = noise_level * np.random.normal(size=data.shape)\n",
    "    return data + noise\n",
    "\n",
    "def scale(data, scaling_factor=1.1):\n",
    "    return data * scaling_factor\n",
    "\n",
    "def time_shift(data, shift_max=2):\n",
    "    shift = np.random.randint(-shift_max, shift_max)\n",
    "    return np.roll(data, shift)\n",
    "\n",
    "def augment_data(X, Y, augmentations=5):\n",
    "    augmented_X, augmented_Y = [], []\n",
    "    for _ in range(augmentations):\n",
    "        for x, y in zip(X, Y):\n",
    "            augmented_X.append(add_noise(x))\n",
    "            augmented_X.append(scale(x))\n",
    "            augmented_X.append(time_shift(x))\n",
    "            augmented_Y.extend([y, y, y])\n",
    "    return np.array(augmented_X), np.array(augmented_Y)\n",
    "\n",
    "# Assuming all_X_normalized and all_Y_numeric are already defined\n",
    "# Augment the dataset\n",
    "augmented_X, augmented_Y = augment_data(all_X_normalized, all_Y_numeric)\n",
    "\n",
    "# Combine original and augmented data\n",
    "final_X = np.vstack((all_X_normalized, augmented_X))\n",
    "final_Y = np.hstack((all_Y_numeric, augmented_Y))\n",
    "\n",
    "# Shuffle the data\n",
    "final_X, final_Y = shuffle(final_X, final_Y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_X, final_Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the models\n",
    "models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Support Vector Machine': SVC(),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Bilayered Neural Network': MLPClassifier(hidden_layer_sizes=(10,),max_iter=1000),\n",
    "    'Trilayered Neural Network': MLPClassifier(hidden_layer_sizes=(10, 10), max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_jobs=-1)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 60.05%\n"
     ]
    }
   ],
   "source": [
    "DTmodel = DecisionTreeClassifier()\n",
    "DTmodel.fit(X_train, y_train)\n",
    "y_pred_DT = DTmodel.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "acc = accuracy_score(y_test, y_pred_DT)\n",
    "\n",
    "# Print the accuracy\n",
    "print(f'Decision Tree Accuracy: {acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 60.03%\n",
      "Naive Bayes Accuracy: 39.42%\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate each model using a for loop\n",
    "for model_name, model in models.items():\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Print the accuracy\n",
    "    print(f'{model_name} Accuracy: {accuracy * 100:.2f}%')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate the models - using multiprocessing \n",
    "def train_and_evaluate(model_name, model, X_train, y_train, X_test, y_test):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return model_name, accuracy\n",
    "\n",
    "# Train and evaluate each model in parallel\n",
    "with Pool(cpu_count()) as pool:\n",
    "    results = pool.starmap(train_and_evaluate, [(name, model, X_train, y_train, X_test, y_test) for name, model in models.items()])\n",
    "\n",
    "# Print the accuracy of each model\n",
    "for model_name, accuracy in results:\n",
    "    print(f'{model_name} Accuracy: {accuracy * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
